# ===== DATASET QA PACK =====
suppressPackageStartupMessages({
library(dplyr)
})
load("dados.rda")  # expects object 'dados' (matrix with date rownames)
stopifnot(is.matrix(dados), !is.null(rownames(dados)))
stopifnot("cpi_total" %in% colnames(dados))
# 0) Basic structure
dates <- rownames(dados)
cat("Rows:", nrow(dados), " Cols:", ncol(dados), "\n")
cat("Head dates:", paste(head(dates,3), collapse=", "), "\n")
cat("Tail dates:", paste(tail(dates,3), collapse=", "), "\n")
# Ensure dates sorted & unique
is_sorted <- all(order(dates) == seq_along(dates))
is_unique <- length(unique(dates)) == length(dates)
cat("Dates sorted? ", is_sorted, " | unique? ", is_unique, "\n")
# 1) NA/Inf and near-zero variance checks
cn <- colnames(dados)
sdv <- apply(dados, 2, sd, na.rm=TRUE)
any_na <- sapply(seq_len(ncol(dados)), function(j) any(is.na(dados[,j])))
any_inf <- sapply(seq_len(ncol(dados)), function(j) any(!is.finite(dados[,j])))
nzv_idx <- which(sdv < 1e-8 | !is.finite(sdv))
if (length(nzv_idx)) {
cat("\nNear-zero/zero-variance columns:\n"); print(cn[nzv_idx])
}
if (any(any_na)) {
cat("\nColumns with NAs:\n"); print(cn[which(any_na)])
}
if (any(any_inf)) {
cat("\nColumns with non-finite values:\n"); print(cn[which(any_inf)])
}
# 2) Duplicate / near-duplicate columns
# Exact duplicates
dup_pairs <- list()
for (j in 1:(ncol(dados)-1)) {
eq <- colSums(abs(dados[, j] - dados[, (j+1):ncol(dados)]) < 1e-12, na.rm=TRUE) == nrow(dados)
if (any(eq)) {
k <- which(eq) + j
dup_pairs[[length(dup_pairs)+1]] <- data.frame(var1=cn[j], var2=cn[k])
}
}
# ===== DATASET QA PACK =====
suppressPackageStartupMessages({
library(dplyr)
})
load("dados.rda")  # expects object 'dados' (matrix with date rownames)
stopifnot(is.matrix(dados), !is.null(rownames(dados)))
stopifnot("cpi_total" %in% colnames(dados))
# 0) Basic structure
dates <- rownames(dados)
cat("Rows:", nrow(dados), " Cols:", ncol(dados), "\n")
cat("Head dates:", paste(head(dates,3), collapse=", "), "\n")
cat("Tail dates:", paste(tail(dates,3), collapse=", "), "\n")
# Ensure dates sorted & unique
is_sorted <- all(order(dates) == seq_along(dates))
is_unique <- length(unique(dates)) == length(dates)
cat("Dates sorted? ", is_sorted, " | unique? ", is_unique, "\n")
# 1) NA/Inf and near-zero variance checks
cn <- colnames(dados)
sdv <- apply(dados, 2, sd, na.rm=TRUE)
any_na <- sapply(seq_len(ncol(dados)), function(j) any(is.na(dados[,j])))
any_inf <- sapply(seq_len(ncol(dados)), function(j) any(!is.finite(dados[,j])))
nzv_idx <- which(sdv < 1e-8 | !is.finite(sdv))
if (length(nzv_idx)) {
cat("\nNear-zero/zero-variance columns:\n"); print(cn[nzv_idx])
}
if (any(any_na)) {
cat("\nColumns with NAs:\n"); print(cn[which(any_na)])
}
if (any(any_inf)) {
cat("\nColumns with non-finite values:\n"); print(cn[which(any_inf)])
}
# 2) Duplicate / near-duplicate columns (robust)
tol <- 1e-10
cn  <- colnames(dados)
p   <- ncol(dados)
# --- Exact (within tol) duplicates ---
dup_pairs <- list()
if (p >= 2) {
for (j in 1:(p-1)) {
M <- dados[, (j+1):p, drop = FALSE]      # keep as matrix
if (ncol(M) == 0) next
v <- as.numeric(dados[, j])
# broadcast v across columns of M and test near-equality
D <- abs(sweep(M, 1, v, "-")) < tol      # rows x (p-j) logical
eq <- colSums(D, na.rm = TRUE) == nrow(dados)
if (any(eq)) {
k <- which(eq) + j
dup_pairs[[length(dup_pairs)+1]] <- data.frame(var1 = cn[j], var2 = cn[k])
}
}
}
if (length(dup_pairs)) {
cat("\nExact duplicate column pairs (within tol):\n")
print(do.call(rbind, dup_pairs))
} else {
cat("\nNo exact duplicates found (within tol).\n")
}
# --- Near-duplicates by correlation ---
C <- suppressWarnings(cor(dados, use = "pairwise.complete.obs"))
near_dup <- which(abs(C) >= 0.999 & upper.tri(C), arr.ind = TRUE)
if (nrow(near_dup)) {
cat("\nNear-duplicate (|corr|>=0.999) pairs:\n")
print(data.frame(var1 = colnames(C)[near_dup[,1]],
var2 = colnames(C)[near_dup[,2]],
corr = C[near_dup]))
} else {
cat("\nNo near-duplicate pairs at |corr| >= 0.999.\n")
}
# 3) Rank check after lagging (approximate)
lag_k <- 4
make_lag <- function(v, L) c(rep(NA, L), v[1:(length(v)-L)])
# build X_lags for non-targets
Xraw <- dados[, setdiff(cn, "cpi_total"), drop=FALSE]
Xlags <- do.call(cbind, lapply(1:lag_k, function(L) {
xs <- apply(Xraw, 2, make_lag, L=L)
colnames(xs) <- paste0(colnames(Xraw), "_L", L)
xs
}))
# y AR lags
y <- as.numeric(dados[, "cpi_total"])
ylag <- sapply(1:lag_k, function(L) make_lag(y, L))
colnames(ylag) <- paste0("y_lag", 1:lag_k)
# contemporaneous PCs sanity: not computed here (depends on initial window),
# but we can check rank of lag-only design after trimming NAs
X_design <- cbind(ylag, Xlags)
keep <- complete.cases(X_design)
Xc <- X_design[keep, , drop=FALSE]
qr_rank <- qr(Xc)$rank
cat("\nLag-only design: rows=", nrow(Xc), " cols=", ncol(Xc), " rank≈", qr_rank, "\n")
# 4) Leakage probe: correlation of X_t with future y_{t+h}
# If many predictors have absurdly high |corr| with y_{t+h} across h, leakage is likely.
check_h <- c(1,3,6,12)
for (h in check_h) {
y_fut <- c(y[(1+h):length(y)], rep(NA, h))
cor_vec <- suppressWarnings(cor(dados[, setdiff(cn, "cpi_total")], y_fut, use="pairwise.complete.obs"))
ord <- order(abs(cor_vec), decreasing=TRUE)
top <- head(data.frame(var=setdiff(cn, "cpi_total")[ord],
corr=cor_vec[ord]), 10)
cat("\nTop 10 |corr|(X_t, y_{t+", h, "})\n", sep="")
print(top, row.names=FALSE)
}
# 5) OOS denominator sanity (how harsh/easy is RW by horizon)
# You’ll need the exact OOS dates you used; if not saved, approximate with last 99
nprev <- 99
idx_oos <- (nrow(dados)-nprev+1):nrow(dados)
y_oos <- y[idx_oos]
for (h in 1:12) {
denom <- sqrt(mean((y_oos - y[idx_oos - h])^2))
cat("h=", h, "  RW denom (RMSE): ", round(denom, 6), "\n")
}
# 6) Transform sanity: are any series implausibly scaled vs cpi_total?
scales <- apply(dados, 2, function(z) median(abs(z), na.rm=TRUE))
scales <- sort(scales, decreasing=TRUE)
cat("\nLargest median|value| by series (top 10):\n")
print(head(scales, 10))
# ===== DATASET QA PACK =====
suppressPackageStartupMessages({
library(dplyr)
})
load("dados.rda")  # expects object 'dados' (matrix with date rownames)
stopifnot(is.matrix(dados), !is.null(rownames(dados)))
stopifnot("cpi_total" %in% colnames(dados))
# 0) Basic structure
dates <- rownames(dados)
cat("Rows:", nrow(dados), " Cols:", ncol(dados), "\n")
cat("Head dates:", paste(head(dates,3), collapse=", "), "\n")
cat("Tail dates:", paste(tail(dates,3), collapse=", "), "\n")
# Ensure dates sorted & unique
is_sorted <- all(order(dates) == seq_along(dates))
is_unique <- length(unique(dates)) == length(dates)
cat("Dates sorted? ", is_sorted, " | unique? ", is_unique, "\n")
# 1) NA/Inf and near-zero variance checks
cn <- colnames(dados)
sdv <- apply(dados, 2, sd, na.rm=TRUE)
any_na <- sapply(seq_len(ncol(dados)), function(j) any(is.na(dados[,j])))
any_inf <- sapply(seq_len(ncol(dados)), function(j) any(!is.finite(dados[,j])))
nzv_idx <- which(sdv < 1e-8 | !is.finite(sdv))
if (length(nzv_idx)) {
cat("\nNear-zero/zero-variance columns:\n"); print(cn[nzv_idx])
}
if (any(any_na)) {
cat("\nColumns with NAs:\n"); print(cn[which(any_na)])
}
if (any(any_inf)) {
cat("\nColumns with non-finite values:\n"); print(cn[which(any_inf)])
}
# 2) Duplicate / near-duplicate columns (robust)
tol <- 1e-10
cn  <- colnames(dados)
p   <- ncol(dados)
# --- Exact (within tol) duplicates ---
dup_pairs <- list()
if (p >= 2) {
for (j in 1:(p-1)) {
M <- dados[, (j+1):p, drop = FALSE]      # keep as matrix
if (ncol(M) == 0) next
v <- as.numeric(dados[, j])
# broadcast v across columns of M and test near-equality
D <- abs(sweep(M, 1, v, "-")) < tol      # rows x (p-j) logical
eq <- colSums(D, na.rm = TRUE) == nrow(dados)
if (any(eq)) {
k <- which(eq) + j
dup_pairs[[length(dup_pairs)+1]] <- data.frame(var1 = cn[j], var2 = cn[k])
}
}
}
if (length(dup_pairs)) {
cat("\nExact duplicate column pairs (within tol):\n")
print(do.call(rbind, dup_pairs))
} else {
cat("\nNo exact duplicates found (within tol).\n")
}
# --- Near-duplicates by correlation ---
C <- suppressWarnings(cor(dados, use = "pairwise.complete.obs"))
near_dup <- which(abs(C) >= 0.999 & upper.tri(C), arr.ind = TRUE)
if (nrow(near_dup)) {
cat("\nNear-duplicate (|corr|>=0.999) pairs:\n")
print(data.frame(var1 = colnames(C)[near_dup[,1]],
var2 = colnames(C)[near_dup[,2]],
corr = C[near_dup]))
} else {
cat("\nNo near-duplicate pairs at |corr| >= 0.999.\n")
}
# 3) Rank check after lagging (approximate)
lag_k <- 4
make_lag <- function(v, L) c(rep(NA, L), v[1:(length(v)-L)])
# build X_lags for non-targets
Xraw <- dados[, setdiff(cn, "cpi_total"), drop=FALSE]
Xlags <- do.call(cbind, lapply(1:lag_k, function(L) {
xs <- apply(Xraw, 2, make_lag, L=L)
colnames(xs) <- paste0(colnames(Xraw), "_L", L)
xs
}))
# y AR lags
y <- as.numeric(dados[, "cpi_total"])
ylag <- sapply(1:lag_k, function(L) make_lag(y, L))
colnames(ylag) <- paste0("y_lag", 1:lag_k)
# contemporaneous PCs sanity: not computed here (depends on initial window),
# but we can check rank of lag-only design after trimming NAs
X_design <- cbind(ylag, Xlags)
keep <- complete.cases(X_design)
Xc <- X_design[keep, , drop=FALSE]
qr_rank <- qr(Xc)$rank
cat("\nLag-only design: rows=", nrow(Xc), " cols=", ncol(Xc), " rank≈", qr_rank, "\n")
# 4) Leakage probe: correlation of X_t with future y_{t+h}
# If many predictors have absurdly high |corr| with y_{t+h} across h, leakage is likely.
check_h <- c(1,3,6,12)
for (h in check_h) {
y_fut <- c(y[(1+h):length(y)], rep(NA, h))
cor_vec <- suppressWarnings(cor(dados[, setdiff(cn, "cpi_total")], y_fut, use="pairwise.complete.obs"))
ord <- order(abs(cor_vec), decreasing=TRUE)
top <- head(data.frame(var=setdiff(cn, "cpi_total")[ord],
corr=cor_vec[ord]), 10)
cat("\nTop 10 |corr|(X_t, y_{t+", h, "})\n", sep="")
print(top, row.names=FALSE)
}
# 5) OOS denominator sanity (how harsh/easy is RW by horizon)
# You’ll need the exact OOS dates you used; if not saved, approximate with last 99
nprev <- 99
idx_oos <- (nrow(dados)-nprev+1):nrow(dados)
y_oos <- y[idx_oos]
for (h in 1:12) {
denom <- sqrt(mean((y_oos - y[idx_oos - h])^2))
cat("h=", h, "  RW denom (RMSE): ", round(denom, 6), "\n")
}
# 6) Transform sanity: are any series implausibly scaled vs cpi_total?
scales <- apply(dados, 2, function(z) median(abs(z), na.rm=TRUE))
scales <- sort(scales, decreasing=TRUE)
cat("\nLargest median|value| by series (top 10):\n")
print(head(scales, 10))
